{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6618ab",
   "metadata": {},
   "source": [
    "# Forward Automatic Differentiation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a9ef8",
   "metadata": {},
   "source": [
    "In **forward-mode automatic differentiation**, we propagate derivatives alongside function evaluations (see figure below). \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"forward_reverse_AD.png\" width=\"600\" height=\"300\">\n",
    "</p>\n",
    "\n",
    "That is, if we have a function $f(x)$,\n",
    "we want both $$f(x) \\quad \\text{and} \\quad f'(x).$$\n",
    "\n",
    "The algorithm this differentiation uses is not **numerical differentiation** nor **symbolic differentiation.** This approch rely on the dual space. Therefore, in order to illustrate this algorithms, we need to define what dual number and dual vectors are.\n",
    "\n",
    "--- \n",
    "## Dual Space and Dual Vector\n",
    "If $ V $ is a vector space (say over $ \\mathbb{R} $), then the **dual space** $ V^* $ is defined as:\n",
    "\n",
    "$$\n",
    "V^* = \\{ w^* : V \\to \\mathbb{R} \\mid w^* \\text{ is linear} \\}.\n",
    "$$\n",
    "\n",
    "That is:\n",
    "\n",
    "Every element $ w^* $ of $ V^* $ is a **linear map** that takes a vector $ v \\in V $ and produces a **scalar**.\n",
    "\n",
    "#### Example:\n",
    "The dot product in the space $ \\mathbb{R}^n $ is a linear functional that map a vector in $ \\mathbb{R}^n $ to $ \\mathbb{R} $ (scalar field) \n",
    "\n",
    "---\n",
    "## Dual Numbers\n",
    "A **dual number** is defined as:\n",
    "\n",
    "$$\n",
    "x + \\epsilon x'\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ x $ is the real (value) part,  \n",
    "- $ x' $ is the “infinitesimal” part, representing the derivative,  \n",
    "- and $ \\epsilon $ is a special symbol such that:\n",
    "\n",
    "$$\n",
    "\\epsilon^2 = 0, \\quad \\epsilon \\neq 0.\n",
    "$$\n",
    "\n",
    "That last rule is what makes it different from complex numbers.\n",
    "\n",
    "\n",
    "### Arithmetic rules of dual numbers\n",
    "\n",
    "Because $ \\epsilon^2 = 0 $, the addition and multiplication can be performed as,\n",
    "\n",
    "| **Operation**     | **Result** |\n",
    "|--------------------|------------|\n",
    "| Addition | $$ (a + b\\epsilon) + (c + d\\epsilon) = (a + c) + (b + d)\\epsilon $$ |\n",
    "| Multiplication | $$ (a + b\\epsilon)(c + d\\epsilon) = ac + (ad + bc)\\epsilon \\quad \\text{(since } \\epsilon^2 = 0) $$ |\n",
    "\n",
    "--- \n",
    "## Forward AD for Uni-Variant Function\n",
    "Forward-mode AD using **dual numbers** is based on the idea:\n",
    "\n",
    "$$\n",
    "\\text{Dual number: } x + \\epsilon x', \\quad \\text{where } \\epsilon^2 = 0\n",
    "$$\n",
    "\n",
    "If you apply a function $f$ to this dual number:\n",
    "\n",
    "$$\n",
    "f(x + \\epsilon x') = f(x) + \\epsilon f'(x)x'\n",
    "$$\n",
    "\n",
    "Then the **value** of $f(x)$ is the real part (called the *primal*),  \n",
    "and the **derivative** $f'(x)x'$ is stored in the $\\epsilon$-part (called the *tangent*).\n",
    "\n",
    "#### Example:\n",
    "Let’s say $f(x) = x^2 + 3x$.\n",
    "\n",
    "Compute $f(x + \\epsilon)$:\n",
    "\n",
    "$$\n",
    "f(x + \\epsilon) = (x + \\epsilon)^2 + 3(x + \\epsilon) = x^2 + 2x\\epsilon + 3x + 3\\epsilon = (x^2 + 3x) + (2x + 3)\\epsilon\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "- The real part = $f(x)$  \n",
    "- The coefficient of $\\epsilon$ = $f'(x).$  \n",
    "\n",
    "So, every variable in the computation becomes a dual number $x + \\epsilon $, and all operations are overloaded so that the \n",
    "$\\epsilon $-parts propagate derivatives automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb6d08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 18.00\n",
      "f'(x) = 9.00\n"
     ]
    }
   ],
   "source": [
    "# Here we will implement this approach in PyTorch.\n",
    "import torch\n",
    "\n",
    "# Assuming  that we have an input point x = 3, and we want to find the derivative of f at x using the computational graph.\n",
    "x_val = torch.tensor(3.0)\n",
    "\n",
    "# Create a dual number (value + infinitesimal) via forward-mode AD\n",
    "with torch.autograd.forward_ad.dual_level():\n",
    "    x = torch.autograd.forward_ad.make_dual(x_val, torch.tensor(1.0))  # 1.0 -> dx/dx = 1, so we define x as, x + ε..\n",
    "    f = x**2 + 3 * x\n",
    "    # Extract the primal (real) and tangent (ε-part) components\n",
    "    f_value , f_derivative  = torch.autograd.forward_ad.unpack_dual(f)\n",
    "\n",
    "print(f\"f(x) = {f_value.item():.2f}\")\n",
    "print(f\"f'(x) = {f_derivative.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c82366",
   "metadata": {},
   "source": [
    "As you see, the result is not symbolic as the derivative was not found symbolically neither numerically. This algorithm is found to be efficient for two reasons, it does not use numerical approximation, and avoid complexity of finding the symbolic differentiation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edb727",
   "metadata": {},
   "source": [
    "# Forward AD for multi-variant Funtion\n",
    "Forward AD for multi-variant functions is also implemented via dual numbers, the difference is here what it computes for a given direction $v$\n",
    " is exactly the **Jacobian–vector product (JVP)**.\n",
    "\n",
    "Formally, for a given $f(\\bf{x})$ and direction $v$,\n",
    "\n",
    "$$\n",
    "\\text{Forward AD gives:} \\quad (f(\\bf{x}), J_f(\\bf{x}) \\cdot v)\n",
    "$$\n",
    "\n",
    "Similarly, this arithmetic computes **both** the function output and its directional derivative at once.\n",
    "\n",
    "---\n",
    "\n",
    "####  Dual numbers represent the JVP computation\n",
    "\n",
    "When we evaluate $f$ on a **dual number vector**\n",
    "\n",
    "$$\n",
    "x + \\epsilon v\n",
    "$$\n",
    "\n",
    "(where $v$ is a direction vector and $\\epsilon^2 = 0$),  \n",
    "we get:\n",
    "\n",
    "$$\n",
    "f(\\bf{x} + \\epsilon v) = f(\\bf{x}) + \\epsilon (J_f(\\bf{x}) \\cdot v)\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "- The **real part** = $f(\\bf{x})$  \n",
    "- The **$\\epsilon$ part** = $J_f(\\bf{x}) \\cdot v$\n",
    "\n",
    "#### Example:\n",
    "Let $f(x, y) = (xy, x + y)$.\n",
    "\n",
    "Before we apply the dual arithmetic to implement the forward AD for this function, we will find the Jacobian matrix symbolically.\n",
    "\n",
    "For this function, the Jacobian matrix is\n",
    "$$\n",
    "J_f(x, y) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial x}(xy) & \\frac{\\partial}{\\partial y}(xy) \\\\\n",
    "\\frac{\\partial}{\\partial x}(x+y) & \\frac{\\partial}{\\partial y}(x+y)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y & x \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Now, for this Jacobian matrix we will implment JVP assuming that the direction is (No dual arithmetic been used),\n",
    "$$ v =\n",
    "\\begin{bmatrix}\n",
    "0  \\\\\n",
    "1 \n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "So, \n",
    "$$\n",
    "J_f(x,y)v = \n",
    "\\begin{bmatrix}\n",
    "y & x \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0  \\\\\n",
    "1 \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "x  \\\\\n",
    "1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "As as result, using this directional derivative we have found total partial derivative with respect to y, which the second column of $J_f(x,y)$.\n",
    "Also, if we compute JVP, with \n",
    "$$\n",
    " v = \n",
    "\\begin{bmatrix}\n",
    "1  \\\\\n",
    "0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "J_f(x,y)v = \n",
    "\\begin{bmatrix}\n",
    "y  \\\\\n",
    "1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Which is the total partial derivative with respect to x.\n",
    "\n",
    "Now, let’s compute the JVP using the dual arithmetic in direction $v = (1, 2)$.\n",
    "\n",
    "We form the dual vector:\n",
    "\n",
    "$$\n",
    "(x, y) = (x + \\varepsilon v_1, y + \\varepsilon v_2).\n",
    "$$\n",
    "\n",
    "Compute $f$:\n",
    "\n",
    "$$\n",
    "f(x, y) = ((x + \\varepsilon v_1)(y + \\varepsilon v_2),\\ (x + \\varepsilon v_1) + (y + \\varepsilon v_2)).\n",
    "$$\n",
    "\n",
    "Expand and collect $\\varepsilon$ terms:\n",
    "\n",
    "$$\n",
    "f(x, y) = (xy + \\varepsilon(v_2 x + v_1 y),\\ x + y + \\varepsilon(v_1 + v_2)).\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "J_f(x, y)\\,v = (v_2 x + v_1 y,\\ v_1 + v_2)\n",
    "$$\n",
    "\n",
    "at $v= (1,2)$ we have,\n",
    "\n",
    "$$J_f(x, y)\\,v = (2x + y, 3)$$\n",
    "\n",
    "That $\\varepsilon$-part is the **JVP**.\n",
    "\n",
    "But, the actual results of this forward mode derivative is not symbolic as I represent it here, the result is numeric vector directly, and for each $(x,y)$ the derivative is been computed automatically along the computational graph. \n",
    "\n",
    "To sum up, the vector $v$ determine the direction of the derivative. That is, if I want partial derivative of the function I will choose this vector as,\n",
    "\n",
    "| Direction $v$ | Meaning | JVP result | Interpretation |\n",
    "|:--------------:|:---------|:------------|:----------------|\n",
    "| $v = (1, 0)$ | perturb $x$, keep $y$ fixed | $(y, 1)$ | derivative w.r.t $x$ |\n",
    "| $v = (0, 1)$ | perturb $y$, keep $x$ fixed | $(x, 1)$ | derivative w.r.t $y$ |\n",
    "| $v = (v_1, v_2)$ | perturb both | $(v_1 y + v_2x,\\, v_1 + v_2)$ | directional derivative along $(v_1, v_2)$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0470e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x, y) = tensor([10.,  7.])\n",
      "df/dx = tensor([5., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Here we will implement this approach in PyTorch.\n",
    "import torch\n",
    "\n",
    "# Assuming  that we have an input point (2,5), and we want to find the derivative of f wrt x using the computational graph.\n",
    "p1 = torch.tensor([2.0,5.0])\n",
    "def f(x):\n",
    "    return torch.stack([\n",
    "        x[0] * x[1],   # f1 = x * y\n",
    "        x[0] + x[1]    # f2 = x + y\n",
    "    ])\n",
    "# Create a dual number (value + infinitesimal) via forward-mode AD\n",
    "with torch.autograd.forward_ad.dual_level():\n",
    "    x = torch.autograd.forward_ad.make_dual(p1, torch.tensor([1.0,0.0]))  # p1 = (x+εv, y+εv), v is with x- direction\n",
    "    evaluation = f(x)\n",
    "    f_value , f_derivative  = torch.autograd.forward_ad.unpack_dual(evaluation)\n",
    "\n",
    "print(\"f(x, y) =\", f_value)\n",
    "print(\"df/dx =\", f_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54b46c",
   "metadata": {},
   "source": [
    "The results of the both examples we presented are obtained using the built in functionality in PyTorch.\n",
    "It maybe useful to see how we can rebuild this functionality from scratch using Algebraic concepts of vector spaces. \n",
    "\n",
    "To construct the **dual number space**, we apply the following theorem to our set $\\mathbb{S}$, which is an extension of the real numbers $\\mathbb{R}$.  \n",
    "Such that, for every $x \\in \\mathbb{R}$, we define an element of $\\mathbb{S}$ as $x + \\epsilon x'$, where $\\epsilon$ is an infinitesimal such that $\\epsilon^2 = 0$.\n",
    "\n",
    "**Theorem:**  \n",
    "A set $V$ is a **vector space** if it is closed under vector addition and scalar multiplication.\n",
    "\n",
    "Before proceeding to implement the `Dual` class in code, we first need to establish the **algebraic arithmetic** of dual numbers.  \n",
    "In the previous table, we have already defined the rules for **addition** and **multiplication**.  \n",
    "What remains is to define the rules for **subtraction** and **division**.\n",
    "\n",
    "Since subtraction directly follows the same structure as addition, we will focus here on deriving the **division rule** for dual numbers.\n",
    "\n",
    "### Derivation of the Division Rule for Dual Numbers\n",
    "\n",
    "We want to compute the quotient of two dual numbers:\n",
    "\n",
    "$$\n",
    "\\frac{a + \\varepsilon a'}{b + \\varepsilon b'}, \\quad \\text{where } \\varepsilon^2 = 0.\n",
    "$$\n",
    "\n",
    "We start by writing the reciprocal as:\n",
    "\n",
    "$$\n",
    "\\frac{1}{b + \\varepsilon b'} \n",
    "= \\frac{1}{b} \\cdot \\frac{1}{1 + \\varepsilon \\frac{b'}{b}}.\n",
    "$$\n",
    "So, we can use the Taylor expansion for $ \\frac{1}{1 + \\varepsilon \\frac{b'}{b}}$.\n",
    "The Taylor expansion is,\n",
    "$$\n",
    "\\frac{1}{1 + x} = 1 - x + x^2 - x^3 + \\dots\n",
    "$$\n",
    "\n",
    "Let $x = \\varepsilon \\frac{b'}{b}$, then all higher-order terms vanish because $\\varepsilon^2 = 0.$\n",
    "Thus, the first-order expansion is **exact**, not approximate:\n",
    "\n",
    "$$\n",
    "\\frac{1}{1 + \\varepsilon \\frac{b'}{b}} = 1 - \\varepsilon \\frac{b'}{b}.\n",
    "$$\n",
    "\n",
    "Now, ubstituting this into the reciprocal expression gives:\n",
    "\n",
    "$$\n",
    "\\frac{1}{b + \\varepsilon b'} \n",
    "= \\frac{1}{b} \\left( 1 - \\varepsilon \\frac{b'}{b} \\right)\n",
    "= \\frac{1}{b} - \\varepsilon \\frac{b'}{b^2}.\n",
    "$$\n",
    "\n",
    "Now, we multiply this by the numerator $a + \\varepsilon a'$:\n",
    "\n",
    "$$\n",
    "(a + \\varepsilon a')\\left(\\frac{1}{b} - \\varepsilon \\frac{b'}{b^2}\\right)\n",
    "= \\frac{a}{b} + \\varepsilon \\left( \\frac{a'}{b} - \\frac{a b'}{b^2} \\right).\n",
    "$$\n",
    "\n",
    "Finally, we simplify the result by combining the real and infinitesimal parts to get:\n",
    "\n",
    "$$\n",
    "\\frac{a + \\varepsilon a'}{b + \\varepsilon b'} \n",
    "= \\frac{a}{b} + \\varepsilon \\frac{a'b - a b'}{b^2}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Connection to the quotient rule in calculus\n",
    "\n",
    "The coefficient of $\\varepsilon$,\n",
    "$$\n",
    "\\frac{a'b - a b'}{b^2},\n",
    "$$\n",
    "is exactly the **quotient rule** from differentiation:\n",
    "\n",
    "$$\n",
    "\\left( \\frac{a}{b} \\right)' = \\frac{a'b - a b'}{b^2}.\n",
    "$$\n",
    "\n",
    "This shows that **dual number algebra inherently encodes the rules of differentiation**.  \n",
    "Hence, when we perform division on dual numbers, the tangent part automatically follows the derivative of a quotient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9022f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# We’ll use double precision for numerical accuracy\n",
    "dtype = torch.float64\n",
    "\n",
    "# 1. Define a Dual number class\n",
    "class Dual:\n",
    "    def __init__(self, val, tan=None):\n",
    "        # val: torch tensor (the actual value)\n",
    "        # tan: torch tensor (the derivative / tangent part)\n",
    "        self.val = torch.as_tensor(val, dtype=dtype) # This is how we define attribute for object\n",
    "        if tan is None:\n",
    "            self.tan = torch.zeros_like(self.val) # only real part (val)\n",
    "        else:\n",
    "            self.tan = torch.as_tensor(tan, dtype=dtype)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.val} + ε {self.tan}\" # Automatically represent our object in readable form\n",
    "\n",
    " \n",
    "    # Define arithmetic operations using dual number algebra \n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Dual) else Dual(other)\n",
    "        return Dual(self.val + other.val, self.tan + other.tan)\n",
    "    __radd__ = __add__ # Since addition is commutitive \n",
    "\n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Dual) else Dual(other)\n",
    "        return Dual(self.val - other.val, self.tan - other.tan)\n",
    "    def __rsub__(self, other):\n",
    "        other = other if isinstance(other, Dual) else Dual(other)\n",
    "        return other - self # The regualr subtraction now already defined.\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Dual) else Dual(other)\n",
    "        # (a + εa')(b + εb') = ab + ε(ab' + a'b)\n",
    "        val = self.val * other.val\n",
    "        tan = self.val * other.tan + self.tan * other.val\n",
    "        return Dual(val, tan)\n",
    "    __rmul__ = __mul__ # Since multiplication is commutitive \n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Dual) else Dual(other)\n",
    "        if torch.all(other.val == 0):\n",
    "           raise ZeroDivisionError(\"Only nonzero denomerator supported.\")\n",
    "        val = self.val / other.val\n",
    "        tan = (self.tan * other.val - self.val * other.tan) / (other.val ** 2)\n",
    "        return Dual(val, tan)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        other = other if isinstance(other, Dual) else Dual(other)\n",
    "        return other / self\n",
    "\n",
    "    def __pow__(self, p):\n",
    "        # Only scalar powers\n",
    "        if isinstance(p, (int, float)):\n",
    "            val = self.val ** p\n",
    "            tan = p * (self.val ** (p - 1)) * self.tan\n",
    "            return Dual(val, tan)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only scalar powers supported.\")\n",
    "\n",
    "    # Accessors helps if I want to call single attribute of my object.\n",
    "    def value(self): \n",
    "        return self.val\n",
    "\n",
    "    def tangent(self):\n",
    "        return self.tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c34293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Dual(1,2)\n",
    "p2 = Dual(2,3)\n",
    "p3 = Dual(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d063ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "tensor(2., dtype=torch.float64)\n",
      "1.0 + ε 2.0\n",
      "3.0 + ε 5.0\n",
      "2.0 + ε 7.0\n",
      "-1.0 + ε -1.0\n",
      "0.0 + ε 2.0\n"
     ]
    }
   ],
   "source": [
    "print(p1.val.item())\n",
    "print(p1.tan)\n",
    "print(p1)\n",
    "print((p1 + p2))\n",
    "print(p1*p2)\n",
    "print(p1 - p2)\n",
    "print(p3 / p1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca5077",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "Only nonzero denomerator supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mp1\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mp3\u001b[49m)\n",
      "Cell \u001b[1;32mIn[18], line 46\u001b[0m, in \u001b[0;36mDual.__truediv__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     44\u001b[0m other \u001b[38;5;241m=\u001b[39m other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Dual) \u001b[38;5;28;01melse\u001b[39;00m Dual(other)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(other\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m---> 46\u001b[0m    \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly nonzero denomerator supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m/\u001b[39m other\u001b[38;5;241m.\u001b[39mval\n\u001b[0;32m     48\u001b[0m tan \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtan \u001b[38;5;241m*\u001b[39m other\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m*\u001b[39m other\u001b[38;5;241m.\u001b[39mtan) \u001b[38;5;241m/\u001b[39m (other\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: Only nonzero denomerator supported."
     ]
    }
   ],
   "source": [
    "print(p1 / p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084c740",
   "metadata": {},
   "source": [
    "### Implment our Uni-Variant Example Using This Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add14061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x): 18.0\n",
      "f'(x): 9.0\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.tensor(3.0, dtype=dtype)\n",
    "\n",
    "# Create dual number with tangent = 1, i.e derivative wrt x\n",
    "x = Dual(x0, torch.tensor(1.0, dtype=dtype))\n",
    "def f_univariate(x):\n",
    "    return x**2 + 3*x\n",
    "# Evaluate\n",
    "f_dual = f_univariate(x) # Now it will use the arithmetic we defined in the class\n",
    "\n",
    "print(\"f(x):\", f_dual.val.item())\n",
    "print(\"f'(x):\", f_dual.tan.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66758348",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# -- Examin Outer Product\n",
    "## First of all we will check how we merge features for single point, mentiond in the paper here.\n",
    "![FeatureMerging1](Feature_Merging_single_point.png)\n",
    "\n",
    "### - Output of the architecture at single point $(x,t)$ $\\in$ $\\mathbb{R}^2$\n",
    "\n",
    "Take:\n",
    "- \\($d = 2$\\) (two coordinates: \\($x, t$\\)) \n",
    "- \\($r = 3$\\) (each body-network outputs a vector of length 3). \n",
    "\n",
    "---\n",
    "\n",
    "**Body-network outputs**\n",
    "\n",
    "$$\n",
    "f^{(\\theta_1)}(x) = \\big(f^{(\\theta_1)}_1(x), f^{(\\theta_1)}_2(x), f^{(\\theta_1)}_3(x)\\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f^{(\\theta_2)}(t) = \\big(f^{(\\theta_2)}_1(t), f^{(\\theta_2)}_2(t), f^{(\\theta_2)}_3(t)\\big)\n",
    "$$\n",
    "\n",
    "At inputs \\($x=0.5, t=0.7$\\):\n",
    "\n",
    "$$\n",
    "f^{(\\theta_1)}(x) = (2, 1, 4), \\quad f^{(\\theta_2)}(t) = (3, 5, 6)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Feature merging (simple product across $i$)**\n",
    "\n",
    "For each feature index \\($j=1,2,3$\\):\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^2 f^{(\\theta_i)}_j(x,t)\n",
    "$$\n",
    "\n",
    "- For \\($j=1:\\; f^{(\\theta_1)}_1(x)\\cdot f^{(\\theta_2)}_1(t) = 2 \\cdot 3 = 6$\\) \n",
    "- For \\($j=2:\\; 1 \\cdot 5 = 5$\\) \n",
    "- For \\($j=3:\\; 4 \\cdot 6 = 24$\\) \n",
    "\n",
    "So after the product step we have this vector of length $r$ `rank`:\n",
    "\n",
    "$$\n",
    "(6, 5, 24)\n",
    "$$\n",
    "\n",
    "**Finally we take the Summation over $j$**\n",
    "\n",
    "$$\n",
    "\\hat{u}(x, t) = 6 + 5 + 24 = 35\n",
    "$$\n",
    "\n",
    "So, for single point in $\\mathbb{R}^2$ we have got corresponding single scalar function output through our SPINN architecture network.\n",
    "Now, what if we have more than single point to pass through the network?. Here where the outer product concept has been used.\n",
    "\n",
    "---\n",
    "### - Output of the architecture at multiple points (3 batches) $(x_1,t_1)$, $(x_2,t_2)$, $(x_3,t_3)$ $\\in$ $\\mathbb{R}^2$\n",
    "![FeatureMerging1](Feature_Merging_multiple_points.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff46bc9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example\n",
    "Consider,\n",
    "- Dimension: $d = 2$, (\\($x, t$\\)) \n",
    "- Batch size: \\($N = 3$\\) (three sampled points along each axis) \n",
    "- Feature size: \\($r = 2$\\) \n",
    "\n",
    "So the discretized solution is:\n",
    "\n",
    "$$\n",
    "\\hat{U} \\in \\mathbb{R}^{3 \\times 3}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Inputs (3 batches, that is three points in $\\mathbb{R}^2$)\n",
    "Sampled coordinates:\n",
    "$$\n",
    "X \\in \\mathbb{R}^{N,d}\n",
    "$$\n",
    "$$\n",
    "X_{:,:} = \\begin{bmatrix}\n",
    "0.1 & 0.2 \\\\\n",
    "0.5 & 0.6 \\\\\n",
    "0.9 & 1.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "X_{:,x} = (0.1, 0.5, 0.9), \\quad X_{:,t} = (0.2, 0.6, 1.0)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Body-network outputs\n",
    "The feature matix $F \\in \\mathbb{R}^{N,r,d}$ where $j= 1,2,...,r$, and $i=1,2,...,d$ in our case $d=2$ and $r=2$ and $N=3$\n",
    "For spatial axis \\(x\\):\n",
    "\n",
    "$$\n",
    "F_{:,:,x} =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 2}\n",
    "$$\n",
    "\n",
    "For time axis \\(t\\):\n",
    "\n",
    "$$\n",
    "F_{:,:,t} =\n",
    "\\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "0 & 3 \\\\\n",
    "4 & 5\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Perform Outer product (per feature $j$)\n",
    "\n",
    "**Feature \\($j=1$\\):**\n",
    "\n",
    "$$\n",
    "F_{:,1,x} = (1,3,5), \\quad F_{:,1,t} = (2,0,4)\n",
    "$$\n",
    "\n",
    "Outer product:\n",
    "\n",
    "$$\n",
    "F_{:,1,x} \\otimes F_{:,1,t} =\n",
    "\\begin{bmatrix}\n",
    "1 \\cdot 2 & 1 \\cdot 0 & 1 \\cdot 4 \\\\\n",
    "3 \\cdot 2 & 3 \\cdot 0 & 3 \\cdot 4 \\\\\n",
    "5 \\cdot 2 & 5 \\cdot 0 & 5 \\cdot 4\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 4 \\\\\n",
    "6 & 0 & 12 \\\\\n",
    "10 & 0 & 20\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Feature \\($j=2$\\):**\n",
    "\n",
    "$$\n",
    "F_{:,2,x} = (2,4,6), \\quad F_{:,2,t} = (1,3,5)\n",
    "$$\n",
    "\n",
    "Outer product:\n",
    "\n",
    "$$\n",
    "F_{:,2,x} \\otimes F_{:,2,t} =\n",
    "\\begin{bmatrix}\n",
    "2 \\cdot 1 & 2 \\cdot 3 & 2 \\cdot 5 \\\\\n",
    "4 \\cdot 1 & 4 \\cdot 3 & 4 \\cdot 5 \\\\\n",
    "6 \\cdot 1 & 6 \\cdot 3 & 6 \\cdot 5\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 & 6 & 10 \\\\\n",
    "4 & 12 & 20 \\\\\n",
    "6 & 18 & 30\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Perform the Summation over $j$\n",
    "\n",
    "$$\n",
    "\\hat{U} =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 4 \\\\\n",
    "6 & 0 & 12 \\\\\n",
    "10 & 0 & 20\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "2 & 6 & 10 \\\\\n",
    "4 & 12 & 20 \\\\\n",
    "6 & 18 & 30\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "4 & 6 & 14 \\\\\n",
    "10 & 12 & 32 \\\\\n",
    "16 & 18 & 50\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**Final Result**\n",
    "The predicted discretized solution is:\n",
    "\n",
    "$$\n",
    "\\hat{U}(x,t) =\n",
    "\\begin{bmatrix}\n",
    "4 & 6 & 14 \\\\\n",
    "10 & 12 & 32 \\\\\n",
    "16 & 18 & 50\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{3 \\times 3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7feab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  6. 14.]\n",
      " [10. 12. 32.]\n",
      " [16. 18. 50.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# apply the outer product using the library NumPy\n",
    "netX = np.array([[1,2],[3,4],[5,6]])\n",
    "netT = np.array([[2,1],[0,3],[4,5]])\n",
    "N,r=3,2\n",
    "output = np.zeros((N,N))\n",
    "for j in range(2):\n",
    "   Fx=netX[:,j]\n",
    "   Ft=netT[:,j]\n",
    "   product = np.outer(Fx,Ft)\n",
    "   output += product\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd58080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  6, 14],\n",
      "        [10, 12, 32],\n",
      "        [16, 18, 50]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# PyTorch implementaion\n",
    "# apply the outer product using the method einsum for CP decomposition (this is more efficient due to avoiding massive looping)\n",
    "netX = torch.tensor([[1,2],[3,4],[5,6]])\n",
    "netT = torch.tensor([[2,1],[0,3],[4,5]])\n",
    "N,r=3,2\n",
    "output = torch.einsum('nj,mj->nm',netX,netT)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442073b4",
   "metadata": {},
   "source": [
    "### Refrences (Outer Product):\n",
    "- [The Paper of Separable Physics Informe Neural Networks](https://arxiv.org/abs/2306.15969)\n",
    "- [Here you find details about tensor decompositions, CP decomposition](https://sci-hub.se/https://doi.org/10.1002/sapm192761164)\n",
    "- [Tensor Decompositions and Applications](https://www.kolda.net/publication/TensorReview.pdf)\n",
    "### Refrences (Forward AD):\n",
    "- - [The Paper of Separable Physics Informe Neural Networks](https://arxiv.org/abs/2306.15969)\n",
    "- [A Hitchhiker’s Guide to Automatic Differentiation](https://arxiv.org/pdf/1411.0583)\n",
    "- [PyTorch document Forward-mode Automatic Differentiation](https://docs.pytorch.org/tutorials/intermediate/forward_ad_usage.html?utm_source=chatgpt.com)\n",
    "- [MIT University Lecture Notes: Forward and Reverse-Mode Automatic Differentiation](https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/mit18_s096iap23_lec08.pdf?utm_source=chatgpt.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
